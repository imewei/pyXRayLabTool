name: Documentation Testing (Optimized)

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'docs/**'
      - 'xraylabtool/**'
      - 'README.md'
      - 'CHANGELOG.md'
      - 'docs/examples/**'
      - '.github/workflows/docs-test*.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'docs/**'
      - 'xraylabtool/**'
      - 'README.md'
      - 'CHANGELOG.md'
      - 'docs/examples/**'
      - '.github/workflows/docs-test*.yml'

concurrency:
  group: docs-test-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHONUNBUFFERED: 1
  PYTHONDONTWRITEBYTECODE: 1
  PIP_DISABLE_PIP_VERSION_CHECK: 1
  PIP_NO_PYTHON_VERSION_WARNING: 1

jobs:
  doctest-validation:
    name: Documentation Code Validation
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: |
          pyproject.toml
          docs/requirements.txt

    - name: Cache documentation dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          docs/_build/.doctrees
          ~/.cache/sphinx
        key: doctest-deps-v2-${{ runner.os }}-${{ hashFiles('pyproject.toml', 'docs/requirements.txt', 'docs/**/*.rst') }}
        restore-keys: |
          doctest-deps-v2-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}
          doctest-deps-v2-${{ runner.os }}-
          doctest-deps-${{ runner.os }}-

    - name: Install documentation dependencies
      uses: nick-fields/retry@v3
      with:
        timeout_minutes: 8
        max_attempts: 3
        retry_on: error
        command: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -e .[docs]

    - name: Validate RST files syntax
      run: |
        echo "üìù Validating RST file syntax..."

        rst_errors=0
        rst_files=$(find docs -name "*.rst" -type f)

        for rst_file in $rst_files; do
          echo "Checking: $rst_file"
          if ! python -c "
          import docutils.core
          import sys
          try:
              with open('$rst_file', 'r') as f:
                  content = f.read()
              docutils.core.publish_doctree(content)
              print('‚úÖ $rst_file: Valid RST syntax')
          except Exception as e:
              print('‚ùå $rst_file: RST syntax error - ', str(e))
              sys.exit(1)
          "; then
            rst_errors=$((rst_errors + 1))
          fi
        done

        if [[ $rst_errors -gt 0 ]]; then
          echo "‚ùå Found $rst_errors RST syntax errors"
          exit 1
        else
          echo "‚úÖ All RST files have valid syntax"
        fi

    - name: Run enhanced doctest with error handling
      run: |
        echo "üß™ Running enhanced doctest validation..."

        # Create a comprehensive doctest runner
        python -c "
        import docutils.core
        import docutils.parsers.rst
        import sphinx.cmd.build
        import sys
        import os
        from pathlib import Path

        # Find all RST files
        rst_files = list(Path('docs').glob('**/*.rst'))
        failed_count = 0

        print(f'Testing {len(rst_files)} RST files...')

        for rst_file in rst_files:
            print(f'Testing: {rst_file}')
            try:
                # Try to build just this file
                result = sphinx.cmd.build.build_main([
                    '-b', 'doctest',
                    '-D', 'extensions=sphinx.ext.doctest',
                    'docs',
                    'docs/_build/doctest',
                    str(rst_file)
                ])
                if result == 0:
                    print(f'‚úÖ {rst_file}: Doctest passed')
                else:
                    print(f'‚ö†Ô∏è {rst_file}: Doctest issues found')
                    failed_count += 1
            except Exception as e:
                print(f'‚ùå {rst_file}: Error during doctest - {e}')
                failed_count += 1

        print(f'\\nDoctest Summary: {len(rst_files) - failed_count} passed, {failed_count} failed')

        # Don't fail the build for doctest issues, just report
        if failed_count > 0:
            print(f'‚ö†Ô∏è {failed_count} files have doctest issues')
        else:
            print('‚úÖ All doctests passed!')
        "
      continue-on-error: true

    - name: Test README code examples with isolation
      run: |
        echo "üìñ Testing README code examples..."

        python -c "
        import re
        import subprocess
        import tempfile
        import os
        import sys

        def test_code_block(code, block_num):
            '''Test a single code block in isolation'''
            try:
                # Create a temporary file
                with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                    # Add necessary imports if not present
                    if 'import' not in code and any(keyword in code for keyword in ['xraylabtool', 'calculate']):
                        f.write('import xraylabtool as xlt\\nimport numpy as np\\n\\n')
                    f.write(code)
                    temp_file = f.name

                # Test syntax
                result = subprocess.run(['python', '-m', 'py_compile', temp_file],
                                       capture_output=True, text=True, timeout=10)

                success = result.returncode == 0
                os.unlink(temp_file)
                return success, result.stderr if not success else None

            except Exception as e:
                return False, str(e)

        # Read README.md
        try:
            with open('README.md', 'r') as f:
                content = f.read()
        except FileNotFoundError:
            print('‚ùå README.md not found')
            sys.exit(1)

        # Extract Python code blocks
        code_blocks = re.findall(r'```python\\n(.*?)\\n```', content, re.DOTALL)

        if not code_blocks:
            print('‚ö†Ô∏è No Python code blocks found in README')
            sys.exit(0)

        print(f'Found {len(code_blocks)} Python code blocks in README')

        failed_blocks = []
        skipped_blocks = []

        for i, code in enumerate(code_blocks, 1):
            # Skip blocks that are just imports or comments
            if not code.strip() or code.strip().startswith('#'):
                skipped_blocks.append(i)
                continue

            # Skip blocks without actual functionality
            if not any(keyword in code for keyword in ['calculate', 'xraylabtool', 'result', '=']):
                skipped_blocks.append(i)
                continue

            success, error = test_code_block(code, i)

            if success:
                print(f'‚úÖ Code block {i}: Syntax valid')
            else:
                print(f'‚ùå Code block {i}: Syntax error')
                if error:
                    print(f'   Error: {error.strip()}')
                failed_blocks.append(i)

        # Summary
        total_tested = len(code_blocks) - len(skipped_blocks)
        passed = total_tested - len(failed_blocks)

        print(f'\\nREADME Code Test Summary:')
        print(f'- Total blocks: {len(code_blocks)}')
        print(f'- Tested: {total_tested}')
        print(f'- Skipped: {len(skipped_blocks)}')
        print(f'- Passed: {passed}')
        print(f'- Failed: {len(failed_blocks)}')

        if failed_blocks:
            print(f'\\n‚ö†Ô∏è Failed blocks: {failed_blocks}')
            # Don't fail the build, just warn
            print('Code blocks have syntax issues - please review locally')
        else:
            print('\\n‚úÖ All testable README examples have valid syntax')
        "
      continue-on-error: true

    - name: Test notebook examples
      run: |
        echo "üìì Testing Jupyter notebook examples..."

        if [[ -d "docs/examples" ]]; then
          notebook_count=0
          failed_notebooks=0

          for notebook in docs/examples/*.ipynb; do
            if [[ -f "$notebook" ]]; then
              notebook_count=$((notebook_count + 1))
              echo "Testing: $notebook"

              # Convert notebook to Python and test syntax
              if jupyter nbconvert --to python --stdout "$notebook" > /tmp/test_notebook.py 2>/dev/null; then
                if python -m py_compile /tmp/test_notebook.py 2>/dev/null; then
                  echo "‚úÖ $notebook: Valid syntax"
                else
                  echo "‚ùå $notebook: Syntax errors"
                  failed_notebooks=$((failed_notebooks + 1))
                fi
              else
                echo "‚ö†Ô∏è $notebook: Could not convert"
                failed_notebooks=$((failed_notebooks + 1))
              fi
            fi
          done

          if [[ $notebook_count -eq 0 ]]; then
            echo "üìì No notebook examples found"
          else
            echo "Notebook Summary: $((notebook_count - failed_notebooks))/$notebook_count passed"
          fi
        else
          echo "üìì No docs/examples directory found"
        fi
      continue-on-error: true

  accessibility-check:
    name: Accessibility Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: Install minimal documentation dependencies
      uses: nick-fields/retry@v3
      with:
        timeout_minutes: 5
        max_attempts: 3
        retry_on: error
        command: |
          python -m pip install --upgrade pip
          pip install -e .[docs]

    - name: Build minimal documentation for accessibility check
      run: |
        echo "üî® Building minimal documentation for accessibility checks..."
        sphinx-build -b html docs docs/_build/html -q
      continue-on-error: true

    - name: Check accessibility features
      run: |
        echo "‚ôø Checking documentation accessibility..."

        # Initialize counters
        total_images=0
        images_with_alt=0
        total_headings=0

        if [[ ! -d "docs/_build/html" ]]; then
          echo "‚ö†Ô∏è HTML documentation not built, skipping accessibility checks"
          exit 0
        fi

        # Check for alt text in images
        echo "üñºÔ∏è Checking image alt text..."

        html_files=$(find docs/_build/html -name "*.html" -type f)

        for html_file in $html_files; do
          # Count total images
          img_count=$(grep -c "<img[^>]*>" "$html_file" 2>/dev/null || echo "0")
          total_images=$((total_images + img_count))

          # Count images with alt text
          alt_count=$(grep -c "<img[^>]*alt=" "$html_file" 2>/dev/null || echo "0")
          images_with_alt=$((images_with_alt + alt_count))
        done

        if [[ $total_images -eq 0 ]]; then
          echo "‚úÖ No images found in documentation"
        elif [[ $images_with_alt -eq $total_images ]]; then
          echo "‚úÖ All $total_images images have alt text"
        else
          missing_alt=$((total_images - images_with_alt))
          echo "‚ö†Ô∏è $missing_alt out of $total_images images missing alt text"
        fi

        # Check heading structure (simplified)
        echo "üìë Checking heading structure..."

        if [[ ${#html_files[@]} -gt 0 ]]; then
          heading_count=$(find docs/_build/html -name "*.html" -exec grep -c "<h[1-6]" {} \; 2>/dev/null | awk '{sum += $1} END {print sum}')
          echo "Found approximately $heading_count headings in HTML files"
        fi

        # Check for accessibility landmarks
        echo "üèóÔ∏è Checking accessibility landmarks..."

        landmark_count=0
        for html_file in $html_files; do
          landmarks=$(grep -c "<nav\|<main\|<header\|<footer\|role=" "$html_file" 2>/dev/null || echo "0")
          landmark_count=$((landmark_count + landmarks))
        done

        if [[ $landmark_count -gt 0 ]]; then
          echo "‚úÖ Found $landmark_count accessibility landmarks"
        else
          echo "‚ö†Ô∏è No accessibility landmarks found"
        fi

        echo "‚ôø Accessibility check completed"

  link-validation:
    name: Link Validation (Basic)
    runs-on: ubuntu-latest
    timeout-minutes: 8
    if: github.event_name == 'push'

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - name: Check internal links in markdown files
      run: |
        echo "üîó Checking internal links in markdown files..."

        markdown_files=$(find . -name "*.md" -not -path "./.git/*" -not -path "./docs/_build/*")
        broken_links=0

        for md_file in $markdown_files; do
          echo "Checking: $md_file"

          # Extract markdown links
          while IFS= read -r link; do
            if [[ $link =~ ^\[.*\]\(([^)]+)\)$ ]]; then
              url="${BASH_REMATCH[1]}"

              # Check internal file links
              if [[ $url =~ ^[^:]+\.(md|rst|html)$ ]]; then
                if [[ ! -f "$url" ]]; then
                  echo "‚ùå Broken internal link: $url in $md_file"
                  broken_links=$((broken_links + 1))
                fi
              fi
            fi
          done < <(grep -o '\[.*\](.*)" "$md_file" || true)
        done

        if [[ $broken_links -eq 0 ]]; then
          echo "‚úÖ No broken internal links found"
        else
          echo "‚ö†Ô∏è Found $broken_links broken internal links"
        fi
      continue-on-error: true

  status-summary:
    name: Documentation Test Summary
    if: always()
    needs: [doctest-validation, accessibility-check, link-validation]
    runs-on: ubuntu-latest

    steps:
    - name: Generate test summary
      run: |
        echo "üìã Documentation Testing Summary:"
        echo "================================="
        echo ""
        echo "Doctest Validation: ${{ needs.doctest-validation.result }}"
        echo "Accessibility Check: ${{ needs.accessibility-check.result }}"
        echo "Link Validation: ${{ needs.link-validation.result }}"
        echo ""

        # Determine overall status
        if [[ "${{ needs.doctest-validation.result }}" == "failure" ]]; then
          echo "‚ùå Documentation testing failed - critical issues found"
          exit 1
        elif [[ "${{ needs.doctest-validation.result }}" == "success" ]]; then
          echo "‚úÖ Documentation testing completed successfully"
        else
          echo "‚ö†Ô∏è Documentation testing completed with warnings"
        fi

    - name: Recommendations
      if: always()
      run: |
        echo ""
        echo "üí° Recommendations for improving documentation:"
        echo "- Run 'make docs' locally to test documentation builds"
        echo "- Use 'sphinx-build -W -b html docs docs/_build/html' for strict checking"
        echo "- Validate RST syntax with 'rstcheck docs/'"
        echo "- Test code examples locally before committing"
        echo "- Ensure all images have descriptive alt text"
